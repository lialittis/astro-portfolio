---
title: Security News – 2025-12-22
date: 2025-12-22
tags: [security, news]
---

## The Hacker News
*Cybersecurity news and insights*

### [Iranian Infy APT Resurfaces with New Malware Activity After Years of Silence](https://thehackernews.com/2025/12/iranian-infy-apt-resurfaces-with-new.html) - December 21, 2025

Threat hunters have discerned new activity associated with an Iranian threat actor known as Infy (aka Prince of Persia), nearly five years after the hacking group was observed targeting victims in Sweden, the Netherlands, and Turkey.
"The scale of Prince of Persia's activity is more significant than we originally anticipated," Tomer Bar, vice president of security research at SafeBreach, said

### [U.S. DOJ Charges 54 in ATM Jackpotting Scheme Using Ploutus Malware](https://thehackernews.com/2025/12/us-doj-charges-54-in-atm-jackpotting.html) - December 20, 2025

The U.S. Department of Justice (DoJ) this week announced the indictment of 54 individuals in connection with a multi-million dollar ATM jackpotting scheme.
The large-scale conspiracy involved deploying malware named Ploutus to hack into automated teller machines (ATMs) across the U.S. and force them to dispense cash. The indicted members are alleged to be part of Tren de Aragua (TdA, Spanish for


## SecurityWeek
*Latest cybersecurity news*

### [Thailand Conference Launches International Initiative to Fight Online Scams](https://www.securityweek.com/thailand-conference-launches-international-initiative-to-fight-online-scams/) - December 19, 2025

<p>Similar pledges to fight scam networks were made by members of the Association of Southeast Asian Nations in the months leading up to the Bangkok conference.</p>
<p>The post <a href="https://www.securityweek.com/thailand-conference-launches-international-initiative-to-fight-online-scams/">Thailand Conference Launches International Initiative to Fight Online Scams</a> appeared first on <a href="https://www.securityweek.com">SecurityWeek</a>.</p>

### [Palo Alto Networks, Google Cloud Strike Multibillion-Dollar AI and Cloud Security Deal](https://www.securityweek.com/palo-alto-networks-google-cloud-strike-multibillion-dollar-ai-and-cloud-security-deal/) - December 19, 2025

<p>The agreement strengthens technical and commercial ties as Palo Alto migrates workloads and adopts Google’s Vertex AI and Gemini models.</p>
<p>The post <a href="https://www.securityweek.com/palo-alto-networks-google-cloud-strike-multibillion-dollar-ai-and-cloud-security-deal/">Palo Alto Networks, Google Cloud Strike Multibillion-Dollar AI and Cloud Security Deal</a> appeared first on <a href="https://www.securityweek.com">SecurityWeek</a>.</p>


## The Hacker News
*Cybersecurity news and insights*

### [Cracked Software and YouTube Videos Spread CountLoader and GachiLoader Malware](https://thehackernews.com/2025/12/cracked-software-and-youtube-videos.html) - December 19, 2025

Cybersecurity researchers have disclosed details of a new campaign that has used cracked software distribution sites as a distribution vector for a new version of a modular and stealthy loader known as CountLoader.
The campaign "uses CountLoader as the initial tool in a multistage attack for access, evasion, and delivery of additional malware families," Cyderes Howler Cell Threat Intelligence


## SecurityWeek
*Latest cybersecurity news*

### [AI Security Firm Ciphero Emerges From Stealth With $2.5 Million in Funding](https://www.securityweek.com/ai-security-firm-ciphero-emerges-from-stealth-with-2-5-million-in-funding/) - December 19, 2025

<p>The startup’s solution captures, verifies, and governs all AI interactions within an enterprise’s environment.</p>
<p>The post <a href="https://www.securityweek.com/ai-security-firm-ciphero-emerges-from-stealth-with-2-5-million-in-funding/">AI Security Firm Ciphero Emerges From Stealth With $2.5 Million in Funding</a> appeared first on <a href="https://www.securityweek.com">SecurityWeek</a>.</p>


## Schneier on Security
*Security news and analysis by Bruce Schneier*

### [AI Advertising Company Hacked](https://www.schneier.com/blog/archives/2025/12/ai-advertising-company-hacked.html) - December 19, 2025

<p>At least some of this is <a href="https://www.404media.co/hack-reveals-the-a16z-backed-phone-farm-flooding-tiktok-with-ai-influencers/">coming to light</a>:</p>
<blockquote><p>Doublespeed, a startup backed by Andreessen Horowitz (a16z) that uses a phone farm to manage at least hundreds of AI-generated social media accounts and promote products has been hacked. The hack reveals what products the AI-generated accounts are promoting, often without the required disclosure that these are advertisements, and allowed the hacker to take control of more than 1,000 smartphones that power the company.</p>
<p>The hacker, who asked for anonymity because he feared retaliation from the company, said he reported the vulnerability to Doublespeed on October 31. At the time of writing, the hacker said he still has access to the company&#8217;s backend, including the phone farm itself. ...</p></blockquote>


## Trail of Bits Blog
*Security research and insights from Trail of Bits*

### [Can chatbots craft correct code?](https://blog.trailofbits.com/2025/12/19/can-chatbots-craft-correct-code/) - December 19, 2025

<p>I recently attended the <a href="https://www.ai.engineer/code">AI Engineer Code Summit</a> in New York, an invite-only gathering of AI leaders and engineers. One theme emerged repeatedly in conversations with attendees building with AI: the belief that we’re approaching a future where developers will <em>never</em> need to look at code again. When I pressed these proponents, several made a similar argument:</p>
<blockquote>
<p>Forty years ago, when high-level programming languages like C became increasingly popular, some of the old guard resisted because C gave you less control than assembly. The same thing is happening now with LLMs.</p>
</blockquote>
<p>On its face, this analogy seems reasonable. Both represent increasing abstraction. Both initially met resistance. Both eventually transformed how we write software. But this analogy really thrashes my cache because it misses a fundamental distinction that matters more than abstraction level: <em><strong>determinism</strong></em>.</p>
<p>The difference between compilers and LLMs isn’t just about control or abstraction. It’s about semantic guarantees. And as I’ll argue, that difference has profound implications for the security and correctness of software.</p>
<h2 id="the-compilers-contract-determinism-and-semantic-preservation">The compiler’s contract: Determinism and semantic preservation</h2>
<p>Compilers have one job: preserve the programmer’s semantic intent while changing syntax. When you write code in C, the compiler transforms it into assembly, but the meaning of your code remains intact. The compiler might choose which registers to use, whether to inline a function, or how to optimize a loop, but it doesn’t change what your program <em>does</em>. If the semantics change unintentionally, that’s not a feature. That’s a compiler bug.</p>
<p>This property, semantic preservation, is the foundation of modern programming. When you write <code>result = x + y</code> in Python, the language guarantees that addition happens. The interpreter might optimize how it performs that addition, but it won’t change what operation occurs. If it did, we’d call that a bug in Python.</p>
<p>The historical progression from assembly to C to Python to Rust maintained this property throughout. Yes, we’ve increased abstraction. Yes, we’ve given up fine-grained control. But we’ve never abandoned determinism. The act of programming remains compositional: you build complex systems from simpler, well-defined pieces, and the composition itself is deterministic and unambiguous.</p>
<p>There are some rare conditions where the abstraction of high-level languages prevents the preservation of the programmer’s semantic intent. For example, cryptographic code needs to run in a constant amount of time over all possible inputs; otherwise, an attacker can use the timing differences as an oracle to do things like brute-force passwords. Properties like “constant time execution” aren’t something most programming languages allow the programmer to specify. <a href="https://blog.trailofbits.com/2025/12/02/introducing-constant-time-support-for-llvm-to-protect-cryptographic-code/">Until very recently</a>, there was no good way to force a compiler to emit constant-time code; developers had to resort to using dangerous inline assembly. But with <a href="https://blog.trailofbits.com/2025/12/02/introducing-constant-time-support-for-llvm-to-protect-cryptographic-code/">Trail of Bits’ new extensions to LLVM</a>, we can now have compilers preserve this semantic property as well.</p>
<p>As I wrote back in 2017 in “<a href="https://www.sultanik.com/blog/AutomationOfAutomation">Automation of Automation</a>,” there are fundamental limits on what we can automate. But those limits don’t eliminate determinism in the tools we’ve built; they simply mean we can’t automatically prove every program correct. Compilers don’t try to prove your program correct; they just faithfully translate it.</p>
<h2 id="why-llms-are-fundamentally-different">Why LLMs are fundamentally different</h2>
<p>LLMs are nondeterministic by design. This isn’t a bug; it’s a feature. But it has consequences we need to understand.</p>
<h3 id="nondeterminism-in-practice">Nondeterminism in practice</h3>
<p>Run the same prompt through an LLM twice, and you’ll likely get different code. Even with temperature set to zero, model updates change behavior. The same request to “add error handling to this function” could mean catching exceptions, adding validation checks, returning error codes, or introducing logging, and the LLM might choose differently each time.</p>
<p>This is fine for creative writing or brainstorming. It&rsquo;s less fine when you need the semantic meaning of your code to be preserved.</p>
<h3 id="the-ambiguous-input-problem">The ambiguous input problem</h3>
<p>Natural language is inherently ambiguous. When you tell an LLM to “fix the authentication bug,” you’re assuming it understands:</p>
<ul>
<li>Which authentication system you’re using</li>
<li>What “bug” means in this context</li>
<li>What “fixed” looks like</li>
<li>Which security properties must be preserved</li>
<li>What your threat model is</li>
</ul>
<p>The LLM will confidently generate code based on what it <em>thinks</em> you mean. Whether that matches what you <em>actually</em> mean is probabilistic.</p>
<h3 id="the-unambiguous-input-problem-which-isnt">The unambiguous input problem (which isn’t)</h3>
<p>“Okay,” you might say, “but what if I give the LLM unambiguous input? What if I say ‘translate this C code to Python’ and provide the exact C code?”</p>
<p>Here&rsquo;s the thing: even that isn’t as unambiguous as it seems. Consider this C code:</p>
<figure class="highlight">
 <pre class="chroma" tabindex="0"><code class="language-c"><span class="line"><span class="cl"><span class="c1">// C code
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">int</span> <span class="nf">increment</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"> <span class="k">return</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre>
</figure>
<p>I asked Claude Opus 4.5 (extended thinking), Gemini 3 Pro, and ChatGPT 5.2 to translate this code to Python, and they all produced the same result:</p>
<figure class="highlight">
 <pre class="chroma" tabindex="0"><code class="language-py"><span class="line"><span class="cl"><span class="c1"># Python code</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">increment</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"> <span class="k">return</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span></span></span></code></pre>
</figure>
<p>It is subtle, but the semantics have changed. In Python, signed integer arithmetic has arbitrary precision. In C, overflowing a signed integer is undefined behavior: it might wrap, might crash, <a href="https://thephd.dev/c-undefined-behavior-and-the-sledgehammer-guideline">might do literally anything</a>. In Python, it’s well defined: you get a larger integer. None of the leading foundation models caught this difference. Why not? It depends on whether they were trained on examples highlighting this distinction, whether they “remember” the difference at inference time, and whether they consider it important enough to flag.</p>
<p>There exist an infinite number of Python programs that would behave identically to the C code for all valid inputs. An LLM is not guaranteed to produce any of them.</p>
<p>In fact, it’s impossible for an LLM to exactly translate the code without knowing how the original C developer <em>expected</em> or <em>intended</em> the C compiler to handle this edge case. Did the developer know that the inputs would never cause the addition to overflow? Or perhaps they inspected the assembly output and concluded that their specific compiler wraps to zero on overflow, and that behavior is required elsewhere in the code?</p>
<h2 id="a-case-study-when-claude-fixed-a-bug-that-wasnt-there">A case study: When Claude “fixed” a bug that wasn’t there</h2>
<p>Let me share a recent experience that crystallizes this problem perfectly.</p>
<p>A developer suspected that a new open-source tool had stolen and open-sourced their code without a license. They decided to use <a href="https://github.com/trailofbits/vendetect">Vendetect</a>, an automated source code plagiarism detection tool I developed at Trail of Bits. Vendetect is designed for exactly this use case: you point it at two Git repos, and it finds portions of one repo that were copied from the other, including the specific offending commits.</p>
<p>When the developer ran Vendetect, it failed with a stack trace.</p>
<p>The developer, reasonably enough, turned to Claude for help. Claude analyzed the code, examined the stack trace, and quickly identified what it <em>thought</em> was the culprit: a complex recursive Python function at the heart of Vendetect’s Git repo analysis. Claude helpfully submitted both a GitHub issue and an extensive pull request “fixing” the bug.</p>
<p>I was assigned to review the PR.</p>
<p>First, I looked at the GitHub issue. It had been months since I’d written that recursive function, and Claude’s explanation seemed plausible! It really did look like a bug. When I checked out the code from the PR, the crash was indeed gone. No more stack trace. Problem solved, right?</p>
<p>Wrong.</p>
<p>Vendetect’s output was now empty. When I ran the unit tests, they were failing. Something was broken.</p>
<p>Now, I know recursion in Python is risky. Python’s stack frames are large enough that you can easily overflow the stack with deep recursion. However, I also knew that the inputs to this particular recursive function were constrained such that it would never recurse more than a few times. Claude either missed this constraint or wasn’t convinced by it. So Claude painfully rewrote the function to be iterative.</p>
<p>And broke the logic in the process.</p>
<p>I reverted to the original code on the <code>main</code> branch and reproduced the crash. After minutes of debugging, I discovered the actual problem: it wasn’t a bug in Vendetect at all.</p>
<p>The developer’s input repository contained two files with the same name but different casing: one started with an uppercase letter, the other with lowercase. Both the developer and I were running macOS, which uses a case-insensitive filesystem by default. When Git tries to operate on a repo with a filename collision on a case-insensitive filesystem, it throws an error. Vendetect faithfully reported this Git error, but followed it with a stack trace to show where in the code the Git error occurred.</p>
<p>I did end up modifying Vendetect to handle this edge case and print a more intelligible error message that wasn’t buried by the stack trace. But the bug that Claude had so confidently diagnosed and “fixed” wasn’t a bug at all. Claude had “fixed” working code and broken actual functionality in the process.</p>
<p>This experience crystallized the problem: <strong>LLMs approach code the way a human would on their first day looking at a codebase: with no context about why things are the way they are.</strong></p>
<p>The recursive function looked risky to Claude because recursion in Python <em>can</em> be risky. Without the context that this particular recursion was bounded by the nature of Git repository structures, Claude made what seemed like a reasonable change. It even “worked” in the sense that the crash disappeared. Only thorough testing revealed that it broke the core functionality.</p>
<p>And here’s the kicker: Claude was <em>confident</em>. The GitHub issue was detailed. The PR was extensive. There was no hedging, no uncertainty. Just like a junior developer who doesn’t know what they don’t know.</p>
<h2 id="the-scale-problem-when-context-matters-most">The scale problem: When context matters most</h2>
<p>LLMs work reasonably well on greenfield projects with clear specifications. A simple web app, a standard CRUD interface, boilerplate code. These are templates the LLM has seen thousands of times. The problem is, these aren’t the situations where developers need the most help.</p>
<p>Consider software architecture like building architecture. A prefabricated shed works well for storage: the requirements are simple, the constraints are standard, and the design can be templated. This is your greenfield web app with a clear spec. LLMs can generate something functional.</p>
<p>But imagine iteratively cobbling together a skyscraper with modular pieces and no cohesive plan from the start. You literally end up with Kowloon Walled City: functional, but unmaintainable.</p>
<p>




 

 




 


 <figure>
 <img alt="Figure 1: Gemini’s idea of what an iteratively constructed skyscraper would look like." height="559" src="https://blog.trailofbits.com/2025/12/19/can-chatbots-craft-correct-code/chatbots-craft-correct-code-image-1_hu_9b7cb33d29b14aa2.webp" width="1024" />
 <figcaption>Figure 1: Gemini’s idea of what an iteratively constructed skyscraper would look like.</figcaption>
 </figure>
</p>
<p>And what about renovating a 100-year-old building? You need to know:</p>
<ul>
<li>Which walls are load-bearing</li>
<li>Where utilities are routed</li>
<li>What building codes applied when it was built</li>
<li>How previous renovations affected the structure</li>
<li>What materials were used and how they’ve aged</li>
</ul>
<p>The architectural plans—the original, deterministic specifications—are essential. You can’t just send in a contractor who looks at the building for the first time and starts swinging a sledgehammer based on what seems right.</p>
<p>Legacy codebases are exactly like this. They have:</p>
<ul>
<li>Poorly documented internal APIs</li>
<li>Brittle dependencies no one fully understands</li>
<li>Historical context that doesn’t fit in any context window</li>
<li>Constraints that aren’t obvious from reading the code</li>
<li>Business logic that emerged from <a href="https://ftrain.medium.com/fun-photoshop-file-format-facts-aa1af8a62702">years of incremental requirements changes and accreted functionality</a></li>
</ul>
<p>When you have a complex system with ambiguous internal APIs, where it’s unclear which service talks to what or for what reason, and the documentation is years out of date and too large to fit in an LLM’s context window, this is exactly when LLMs are most likely to confidently do the wrong thing.</p>
<p>The Vendetect story is a microcosm of this problem. The context that mattered—that the recursion was bounded by Git’s structure, that the real issue was a filesystem quirk—wasn’t obvious from looking at the code. Claude filled in the gaps with seemingly reasonable assumptions. Those assumptions were wrong.</p>
<h2 id="the-path-forward-formal-verification-and-new-frameworks">The path forward: Formal verification and new frameworks</h2>
<p>I’m not arguing against LLM coding assistants. In my extensive use of LLM coding tools, both for code generation and bug finding, I’ve found them genuinely useful. They excel at generating boilerplate code, suggesting approaches, serving as a rubber duck for debugging, and summarizing code. The productivity gains are real.</p>
<p>But we need to be clear-eyed about their fundamental limitations.</p>
<h3 id="where-llms-work-well-today">Where LLMs work well today</h3>
<p>LLMs are most effective when you have:</p>
<ul>
<li>Clean, well-documented codebases with idiomatic code</li>
<li>Greenfield projects</li>
<li>Excellent test coverage that catches errors immediately</li>
<li>Tasks where errors are quickly obvious (it crashes, the output is wrong), allowing the LLM to iteratively climb toward the goal</li>
<li>Pair-programming style review by experienced developers who understand the context</li>
<li>Clear, unambiguous specifications written by experienced developers</li>
</ul>
<p>The last two are absolutely necessary for success, but are often not sufficient. In these environments, LLMs can accelerate development. The generated code might not be perfect, but errors are caught quickly and the cost of iteration is low.</p>
<h3 id="what-we-need-to-build">What we need to build</h3>
<p>If the ultimate goal is to raise the level of abstraction for developers <em>above</em> reviewing code, we will need these frameworks and practices:</p>
<p><strong>Formal verification frameworks for LLM output.</strong> We will need tools that can prove semantic preservation—that the LLM’s changes maintain the intended behavior of the code. This is hard, but it’s not impossible. We already have formal methods for certain domains; we need to extend them to cover LLM-generated code.</p>
<p><strong>Better ways to encode context and constraints.</strong> LLMs need more than just the code; they need to understand the invariants, the assumptions, the historical context. We need better ways to capture and communicate this.</p>
<p><strong>Testing frameworks that go beyond “does it crash?”</strong> We need to test semantic correctness, not just syntactic validity. Does the code do what it’s supposed to do? Are the security properties maintained? Are the performance characteristics acceptable? Unit tests are not enough.</p>
<p><strong>Metrics for measuring semantic correctness.</strong> “It compiles” isn’t enough. Even “it passes tests” isn’t enough. We need ways to quantify whether the semantics have been preserved.</p>
<p><strong>Composable building blocks that are secure by design.</strong> Instead of allowing the LLM to write arbitrary code, we will need the LLM to instead build with modular, composable building blocks that have been verified as secure. A bit like how industrial supplies have been commoditized into Lego-like parts. Need a NEMA 23 square body stepper motor with a D profile shaft? No need to design and build it yourself—you can buy a commercial-off-the-shelf motor from any of a dozen different manufacturers and they will all bolt into your project just as well. Likewise, LLMs shouldn’t be implementing their own authentication flows. They should be orchestrating pre-made authentication modules.</p>
<h3 id="the-trust-model">The trust model</h3>
<p>Until we have these frameworks, we need a clear mental model for LLM output: <strong>Treat it like code from a junior developer who’s seeing the codebase for the first time.</strong></p>
<p>That means:</p>
<ul>
<li>Always review thoroughly</li>
<li>Never merge without testing</li>
<li>Understand that “looks right” doesn&rsquo;t mean “is right”</li>
<li>Remember that LLMs are confident even when wrong</li>
<li>Verify that the solution solves the actual problem, not a plausible-sounding problem</li>
</ul>
<p>As a probabilistic system, there’s always a chance an LLM will introduce a bug or misinterpret its prompt. (These are really the same thing.) How small does that probability need to be? Ideally, it would be smaller than a human’s error rate. We’re not there yet, not even close.</p>
<h2 id="conclusion-embracing-verification-in-the-age-of-ai">Conclusion: Embracing verification in the age of AI</h2>
<p>The fundamental computational limitations on automation haven’t changed since I wrote about them in 2017. What has changed is that we now have tools that make it easier to generate incorrect code confidently and at scale.</p>
<p>When we moved from assembly to C, we didn’t abandon determinism; we built compilers that guaranteed semantic preservation. As we move toward LLM-assisted development, we need similar guarantees. But the solution isn’t to reject LLMs! They offer real productivity gains for certain tasks. We just need to remember that their output is only as trustworthy as code from someone seeing the codebase for the first time. Just as we wouldn’t merge a PR from a new developer without review and testing, we can’t treat LLM output as automatically correct.</p>
<p>If you’re interested in formal verification, automated testing, or building more trustworthy AI systems, <a href="https://www.trailofbits.com/contact/">get in touch</a>. At Trail of Bits, we’re working on exactly these problems, and we’d love to hear about your experiences with LLM coding tools, both the successes and the failures. Because right now, we’re all learning together what works and what doesn’t. And the more we share those lessons, the better equipped we&rsquo;ll be to build the verification frameworks we need.</p>


## SecurityWeek
*Latest cybersecurity news*

### [University of Sydney Data Breach Affects 27,000 Individuals](https://www.securityweek.com/university-of-sydney-data-breach-affects-27000-individuals/) - December 19, 2025

<p>Downloaded from a code library, the information pertains to current and former staff and affiliates, and to alumni and students.</p>
<p>The post <a href="https://www.securityweek.com/university-of-sydney-data-breach-affects-27000-individuals/">University of Sydney Data Breach Affects 27,000 Individuals </a> appeared first on <a href="https://www.securityweek.com">SecurityWeek</a>.</p>

### [‘Kimwolf’ Android Botnet Ensnares 1.8 Million Devices](https://www.securityweek.com/kimwolf-android-botnet-ensnares-1-8-million-devices/) - December 19, 2025

<p>Linked to the Aisuru IoT botnet, Kimwolf was seen launching over 1.7 billion DDoS attack commands and increasing its C&#038;C domain’s popularity.</p>
<p>The post <a href="https://www.securityweek.com/kimwolf-android-botnet-ensnares-1-8-million-devices/">&#8216;Kimwolf&#8217; Android Botnet Ensnares 1.8 Million Devices</a> appeared first on <a href="https://www.securityweek.com">SecurityWeek</a>.</p>


## The Hacker News
*Cybersecurity news and insights*

### [WatchGuard Warns of Active Exploitation of Critical Fireware OS VPN Vulnerability](https://thehackernews.com/2025/12/watchguard-warns-of-active-exploitation.html) - December 19, 2025

WatchGuard has released fixes to address a critical security flaw in Fireware OS that it said has been exploited in real-world attacks.
Tracked as CVE-2025-14733 (CVSS score: 9.3), the vulnerability has been described as a case of out-of-bounds write affecting the iked process that could allow a remote unauthenticated attacker to execute arbitrary code.
"This vulnerability affects both the

### [Nigeria Arrests RaccoonO365 Phishing Developer Linked to Microsoft 365 Attacks](https://thehackernews.com/2025/12/nigeria-arrests-raccoono365-phishing.html) - December 19, 2025

Authorities in Nigeria have announced the arrest of three "high-profile internet fraud suspects" who are alleged to have been involved in phishing attacks targeting major corporations, including the main developer behind the RaccoonO365 phishing-as-a-service (PhaaS) scheme.
The Nigeria Police Force National Cybercrime Centre (NPF–NCCC) said investigations conducted in collaboration with

### [New UEFI Flaw Enables Early-Boot DMA Attacks on ASRock, ASUS, GIGABYTE, MSI Motherboards](https://thehackernews.com/2025/12/new-uefi-flaw-enables-early-boot-dma.html) - December 19, 2025

Certain motherboard models from vendors like ASRock, ASUSTeK Computer, GIGABYTE, and MSI are affected by a security vulnerability that leaves them susceptible to early-boot direct memory access (DMA) attacks across architectures that implement a Unified Extensible Firmware Interface (UEFI) and input–output memory management unit (IOMMU).
UEFI and IOMMU are designed to enforce a security

